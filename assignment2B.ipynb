{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
        "Please refer to the `README.pdf` for full laboratory instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
        "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
        "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
        "\n",
        "### Your Tasks\n",
        "1. **Preprocess the data**: Handle missing values and normalize features\n",
        "2. **Implement k-NN** with L2 distance\n",
        "3. **Evaluate** on both datasets for different values of k\n",
        "4. **Discuss** your results\n",
        "\n",
        "### Datasets\n",
        "The data files are located in the `credit 2017/` folder:\n",
        "- `lenses.training`, `lenses.testing`\n",
        "- `crx.data.training`, `crx.data.testing`\n",
        "- `crx.names` (describes the features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library declarations\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenses - Train: (18, 3), Test: (6, 3)\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "DATA_PATH = \"credit 2017/\"\n",
        "\n",
        "# Load Lenses data\n",
        "def load_lenses_data():\n",
        "    \"\"\"Load the lenses dataset.\"\"\"\n",
        "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
        "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
        "    \n",
        "    # First column is ID, last column is label\n",
        "    X_train = train_data[:, 1:-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, 1:-1]\n",
        "    y_test = test_data[:, -1]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load Credit Approval data\n",
        "def load_credit_data():\n",
        "    \"\"\"\n",
        "    Load the Credit Approval dataset.\n",
        "    Note: This dataset contains missing values (?) and mixed types.\n",
        "    You will need to preprocess it.\n",
        "    \"\"\"\n",
        "    # TODO: Implement data loading\n",
        "    # The data is comma-separated\n",
        "    # Missing values are marked with '?'\n",
        "    # Last column is the label ('+' or '-')\n",
        "    pass\n",
        "\n",
        "# Test loading lenses data\n",
        "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
        "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Preprocessing\n",
        "For the Credit Approval dataset, you need to:\n",
        "1. **Handle missing values** (marked with '?'):\n",
        "   - Categorical features: replace with mode/median\n",
        "   - Numerical features: replace with label-conditioned mean\n",
        "2. **Normalize features** using z-scaling:\n",
        "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
        "\n",
        "Document exactly how you handle each feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_credit_data(train_file, test_file):\n",
        "    \"\"\"\n",
        "    Preprocess the Credit Approval dataset.\n",
        "    \n",
        "    Steps:\n",
        "    1. Load the data\n",
        "    2. Handle missing values\n",
        "    3. Encode categorical variables\n",
        "    4. Normalize numerical features\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, y_train, X_test, y_test : numpy arrays\n",
        "    \"\"\"\n",
        "    # TODO: Implement preprocessing\n",
        "    # Hint: Read crx.names to understand the features\n",
        "    # Feature types (from crx.names):\n",
        "    # A1: categorical (b, a)\n",
        "    # A2: continuous\n",
        "    # A3: continuous\n",
        "    # A4: categorical (u, y, l, t)\n",
        "    # A5: categorical (g, p, gg)\n",
        "    # A6: categorical (c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff)\n",
        "    # A7: categorical (v, h, bb, j, n, z, dd, ff, o)\n",
        "    # A8: continuous\n",
        "    # A9: categorical (t, f)\n",
        "    # A10: categorical (t, f)\n",
        "    # A11: continuous\n",
        "    # A12: categorical (t, f)\n",
        "    # A13: categorical (g, p, s)\n",
        "    # A14: continuous\n",
        "    # A15: continuous\n",
        "    # A16: label (+, -)\n",
        "\n",
        "    # 1. Load the data\n",
        "    # Which features are categorical vs continuous in \n",
        "    categorical_indices= [0, 3, 4, 5, 6, 8, 9, 11, 12]\n",
        "    continuous_indices= [1, 2, 7, 10, 13, 14]\n",
        "    \n",
        "    #load the data into an two empty array of data and their labels\n",
        "    train_data= []\n",
        "    train_labels= []\n",
        "\n",
        "    # open the training file, read it and sort it. I used the following website for function help on open(). Last column is the label and first 15 columns are the features: https://www.w3schools.com/python/ref_func_open.asp\n",
        "    f= open(train_file, 'r')\n",
        "    for line in f:\n",
        "        fields= line.strip().split(',')\n",
        "        train_labels.append(fields[-1])\n",
        "        train_data.append(fields[:-1])\n",
        "    f.close()\n",
        "    \n",
        "    #create empty array for the tested data\n",
        "    test_data= []\n",
        "    test_labels= []\n",
        "\n",
        "    #read the tested data file and sort it\n",
        "    f= open(test_file, 'r')\n",
        "    for line in f:\n",
        "        fields= line.strip().split(',')\n",
        "        test_labels.append(fields[-1])\n",
        "        test_data.append(fields[:-1])\n",
        "    f.close()\n",
        "    \n",
        "\n",
        "    # 2. Handle missing values\n",
        "    #dataset uses ? to mark a missing or unkown value, so we have to replace them in the arrays\n",
        "\n",
        "    #loop through each feature column\n",
        "    for feat_idx in range(15):\n",
        "        #is there a ? in the training data?\n",
        "        has_missing = any(row[feat_idx] == '?' for row in train_data)\n",
        "        if not has_missing:\n",
        "            # If there are no ?, skip the feature column\n",
        "            continue\n",
        "        \n",
        "        #if the feature is categorical\n",
        "        if feat_idx in categorical_indices:\n",
        "            # find every value that is not ?\n",
        "            values = [row[feat_idx] for row in train_data if row[feat_idx] != '?']\n",
        "            mode_value = Counter(values).most_common(1)[0][0]\n",
        "            #replace missing values with mode or most common\n",
        "            for row in train_data:\n",
        "                if row[feat_idx] == '?':\n",
        "                    row[feat_idx] = mode_value\n",
        "        \n",
        "        #if the feature is continuous, replace the ? with the mean\n",
        "        else:\n",
        "            #find values where the label is + and not ?\n",
        "            plus_values= [float(train_data[i][feat_idx]) \n",
        "                          for i in range(len(train_data)) \n",
        "                          if train_data[i][feat_idx] != '?' and train_labels[i] == '+']\n",
        "            plus_mean= np.mean(plus_values)\n",
        "            \n",
        "            #same thing but for -\n",
        "            minus_values = [float(train_data[i][feat_idx]) \n",
        "                           for i in range(len(train_data)) \n",
        "                           if train_data[i][feat_idx] != '?' and train_labels[i] == '-']\n",
        "            minus_mean = np.mean(minus_values)\n",
        "            \n",
        "            #replace ? depending on the label for that row that the ? belongs in\n",
        "            for i in range(len(train_data)):\n",
        "                if train_data[i][feat_idx] == '?':\n",
        "                    if train_labels[i] == '+':\n",
        "                        train_data[i][feat_idx] = str(plus_mean)\n",
        "                    else:\n",
        "                        train_data[i][feat_idx] = str(minus_mean)\n",
        "    \n",
        "    # use the statistics from training data to fix it by looping by column\n",
        "    for feat_idx in range(15):\n",
        "        #check if there are missing values in the column\n",
        "        has_missing= any(row[feat_idx] == '?' for row in test_data)\n",
        "        if not has_missing:\n",
        "            continue\n",
        "        \n",
        "        #see if the column contains letters\n",
        "        if feat_idx in categorical_indices:\n",
        "            # Use mode from training data\n",
        "            values= [row[feat_idx] for row in train_data if row[feat_idx] != '?']\n",
        "            #find mode so we can replace the missing values in the test data with it\n",
        "            mode_value= Counter(values).most_common(1)[0][0]\n",
        "            \n",
        "            #replace missing values with the mode\n",
        "            for row in test_data:\n",
        "                if row[feat_idx] == '?':\n",
        "                    row[feat_idx] = mode_value\n",
        "        \n",
        "        # for continous features\n",
        "        else:\n",
        "            # compute mean for +\n",
        "            plus_values= [float(train_data[i][feat_idx]) \n",
        "                          for i in range(len(train_data)) \n",
        "                          if train_data[i][feat_idx] != '?' and train_labels[i] == '+']\n",
        "            plus_mean= np.mean(plus_values)\n",
        "            \n",
        "            #now for -\n",
        "            minus_values= [float(train_data[i][feat_idx]) \n",
        "                           for i in range(len(train_data)) \n",
        "                           if train_data[i][feat_idx] != '?' and train_labels[i] == '-']\n",
        "            minus_mean= np.mean(minus_values)\n",
        "            \n",
        "            #replace missing values to set average value to whatever the label is\n",
        "            for i in range(len(test_data)):\n",
        "                if test_data[i][feat_idx] == '?':\n",
        "                    if test_labels[i] == '+':\n",
        "                        test_data[i][feat_idx] = str(plus_mean)\n",
        "                    else:\n",
        "                        test_data[i][feat_idx] = str(minus_mean)\n",
        "    \n",
        "    #3. Encode categorical variables\n",
        "    \n",
        "    #create encoders to store a mapping for each categorical feature. https://www.w3schools.com/python/ref_string_encode.asp\n",
        "    encoders = {}\n",
        "    for feat_idx in categorical_indices:\n",
        "        #goes through every row and grabs the values in the columns, sorts alphabetically and removes duplicates.\n",
        "        unique_values= sorted(set(row[feat_idx] for row in train_data))\n",
        "        #creates like a dictionary\n",
        "        encoders[feat_idx]= {val: i for i, val in enumerate(unique_values)}\n",
        "    \n",
        "    # encode training data\n",
        "    for row in train_data:\n",
        "        for feat_idx in categorical_indices:\n",
        "            #take text value in the column, look it up in the encoder, and replace with the number\n",
        "            row[feat_idx]= encoders[feat_idx][row[feat_idx]]\n",
        "    \n",
        "    #encode test data\n",
        "    for row in test_data:\n",
        "        for feat_idx in categorical_indices:\n",
        "            if row[feat_idx] in encoders[feat_idx]:\n",
        "                row[feat_idx] = encoders[feat_idx][row[feat_idx]]\n",
        "            else:\n",
        "                row[feat_idx] = len(encoders[feat_idx])\n",
        "    \n",
        "    #Convert to arrays\n",
        "    X_train = np.array(train_data, dtype=float)\n",
        "    X_test = np.array(test_data, dtype=float)\n",
        "    \n",
        "    # Convert + to 1 and - to 0\n",
        "    y_train = np.array([1 if label == '+' else 0 for label in train_labels])\n",
        "    y_test = np.array([1 if label == '+' else 0 for label in test_labels])\n",
        "    \n",
        "    #4. Normalize numerical features with next function\n",
        "    X_train, X_test = z_normalize(X_train, X_test, continuous_indices)\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "def z_normalize(X_train, X_test, feature_indices):\n",
        "    \"\"\"\n",
        "    Apply z-score normalization to specified features.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train, X_test : numpy arrays\n",
        "    feature_indices : list of indices for numerical features\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train_normalized, X_test_normalized : numpy arrays\n",
        "    \"\"\"\n",
        "    # TODO: Implement z-normalization\n",
        "    X_train_norm= X_train.copy()\n",
        "    X_test_norm= X_test.copy()\n",
        "\n",
        "    for feat_idx in feature_indices:\n",
        "        #calculate mean and standard deviation from training data\n",
        "        mean = np.mean(X_train[:, feat_idx])\n",
        "        std = np.std(X_train[:, feat_idx])\n",
        "        \n",
        "        # make sure all numbers are real\n",
        "        if std < 0:\n",
        "            std = 1.0\n",
        "        \n",
        "        # Apply equation given above for training data and test\n",
        "        X_train_norm[:, feat_idx]= (X_train[:, feat_idx] - mean)/ std\n",
        "        X_test_norm[:, feat_idx]= (X_test[:, feat_idx] - mean)/ std\n",
        "    \n",
        "    return X_train_norm, X_test_norm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Implement k-NN Classifier\n",
        "Implement k-NN with L2 (Euclidean) distance:\n",
        "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
        "\n",
        "For **categorical attributes**, use:\n",
        "- Distance = 1 if values are different\n",
        "- Distance = 0 if values are the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l2_distance(a, b):\n",
        "    \"\"\"\n",
        "    Compute L2 (Euclidean) distance between two vectors.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    a, b : numpy arrays of same shape\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    distance : float\n",
        "    \"\"\"\n",
        "    # TODO: Implement L2 distance\n",
        "    # using formula above\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array of shape (n_train, n_features)\n",
        "    y_train : numpy array of shape (n_train,)\n",
        "    X_test : numpy array of shape (n_test, n_features)\n",
        "    k : int, number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array of shape (n_test,)\n",
        "    \"\"\"\n",
        "    # TODO: Implement k-NN prediction\n",
        "    # For each test sample:\n",
        "    #   1. Compute distance to all training samples\n",
        "    #   2. Find k nearest neighbors\n",
        "    #   3. Predict using majority voting\n",
        "    pass\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float (between 0 and 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement accuracy computation\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions: [0 1]\n",
            "Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "TASK 2: k-NN IMPLEMENTATION\n",
        "Simple and straightforward implementations\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def l2_distance(a, b):\n",
        "    \"\"\"\n",
        "    Compute L2 (Euclidean) distance between two vectors.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    a, b : numpy arrays of same shape\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    distance : float\n",
        "    \"\"\"\n",
        "    # L2 distance formula: sqrt(sum((a_i - b_i)^2))\n",
        "    return np.sqrt(np.sum((a - b) ** 2))\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array of shape (n_train, n_features)\n",
        "    y_train : numpy array of shape (n_train,)\n",
        "    X_test : numpy array of shape (n_test, n_features)\n",
        "    k : int, number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array of shape (n_test,)\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    \n",
        "    # For each test sample\n",
        "    for test_sample in X_test:\n",
        "        # Step 1: Compute distance to all training samples\n",
        "        distances = []\n",
        "        for i in range(len(X_train)):\n",
        "            dist = l2_distance(test_sample, X_train[i])\n",
        "            distances.append((dist, y_train[i]))\n",
        "        \n",
        "        # Step 2: Find k nearest neighbors\n",
        "        # Sort by distance and take first k\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        k_nearest = distances[:k]\n",
        "        \n",
        "        # Step 3: Predict using majority voting\n",
        "        # Get the labels of k nearest neighbors\n",
        "        k_nearest_labels = [label for (dist, label) in k_nearest]\n",
        "        \n",
        "        # Find most common label\n",
        "        most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
        "        predictions.append(most_common)\n",
        "    \n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float (between 0 and 1)\n",
        "    \"\"\"\n",
        "    # Count how many predictions match the true labels\n",
        "    correct = np.sum(y_true == y_pred)\n",
        "    total = len(y_true)\n",
        "    \n",
        "    return correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Simple test\n",
        "    X_train = np.array([[1, 2], [2, 3], [3, 4], [5, 6]])\n",
        "    y_train = np.array([0, 0, 1, 1])\n",
        "    X_test = np.array([[2, 2], [4, 5]])\n",
        "    \n",
        "    predictions = knn_predict(X_train, y_train, X_test, k=3)\n",
        "    print(f\"Predictions: {predictions}\")\n",
        "    \n",
        "    # Test accuracy\n",
        "    y_true = np.array([0, 1])\n",
        "    acc = compute_accuracy(y_true, predictions)\n",
        "    print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Evaluate on Lenses Dataset\n",
        "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 1.0000\n",
            "k=3: Accuracy = 1.0000\n",
            "k=5: Accuracy = 1.0000\n",
            "k=7: Accuracy = 1.0000\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Lenses dataset\n",
        "# Try different values of k (e.g., 1, 3, 5, 7)\n",
        "\n",
        "k_values = [1, 3, 5, 7]\n",
        "lenses_results = []\n",
        "# \n",
        "for k in k_values:\n",
        "     predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
        "     accuracy = compute_accuracy(y_test_lenses, predictions)\n",
        "     lenses_results.append((k, accuracy))\n",
        "     print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluate on Credit Approval Dataset\n",
        "First preprocess the data, then evaluate k-NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Preprocess Credit Approval data\n",
        "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
        "     DATA_PATH + \"crx.data.training\",\n",
        "     DATA_PATH + \"crx.data.testing\"\n",
        " )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 0.7464\n",
            "k=3: Accuracy = 0.7826\n",
            "k=5: Accuracy = 0.8043\n",
            "k=7: Accuracy = 0.7971\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Credit Approval dataset\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "# \n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "### Results Table\n",
        "\n",
        "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
        "|---------|-----|-----|-----|-----|\n",
        "| Lenses | ? | ? | ? | ? |\n",
        "| Credit Approval | ? | ? | ? | ? |\n",
        "\n",
        "### Discussion\n",
        "*Answer these questions:*\n",
        "1. Which value of k works best for each dataset? Why do you think that is?\n",
        "2. How did preprocessing affect your results on the Credit Approval dataset?\n",
        "3. What are the trade-offs of using different values of k?\n",
        "4. What did you learn from this exercise?\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
